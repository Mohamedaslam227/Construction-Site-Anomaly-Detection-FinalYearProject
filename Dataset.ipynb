{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING SINGLE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_labels(label_file, class_mapping):\n",
    "    \"\"\"\n",
    "    Update the class labels in the label file by using a class mapping.\n",
    "    \n",
    "    Args:\n",
    "        label_file (str): Path to the label file.\n",
    "        class_mapping (dict): Dictionary mapping original class labels to new ones.\n",
    "    \n",
    "    Returns:\n",
    "        str: Updated content of the label file.\n",
    "    \"\"\"\n",
    "    updated_content = []\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            # Update the class label using the mapping\n",
    "            class_label = class_mapping[int(parts[0])]\n",
    "            updated_content.append(f\"{class_label} \" + \" \".join(parts[1:]))\n",
    "    \n",
    "    return \"\\n\".join(updated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_yaml_file(output_dir, num_classes, class_names):\n",
    "    \"\"\"\n",
    "    Create a YAML configuration file for the combined dataset.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Output directory path for the combined dataset.\n",
    "        num_classes (int): Total number of classes in the combined dataset.\n",
    "        class_names (list): List of class names.\n",
    "    \"\"\"\n",
    "    yaml_content = f\"\"\"\n",
    "# Combined Dataset Configuration\n",
    "path: {output_dir}\n",
    "train: {output_dir}/train/images\n",
    "val: {output_dir}/val/images\n",
    "test: {output_dir}/test/images\n",
    "\n",
    "# Number of classes\n",
    "nc: {num_classes}\n",
    "\n",
    "# Class names\n",
    "names: [{', '.join([f\"'{name}'\" for name in class_names])}]\n",
    "\"\"\"\n",
    "    yaml_path = os.path.join(output_dir, \"dataset_config.yaml\")\n",
    "    with open(yaml_path, 'w') as yaml_file:\n",
    "        yaml_file.write(yaml_content)\n",
    "    print(f\"YAML configuration file created at: {yaml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_datasets(base_dir, dataset_dirs, num_classes_list, class_names, output_dir):\n",
    "    \"\"\"\n",
    "    Combine multiple datasets into a single dataset with unique class labels.\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): Base directory path where datasets are located.\n",
    "        dataset_dirs (list): List of dataset directories to combine.\n",
    "        num_classes_list (list): List containing the number of classes for each dataset.\n",
    "        class_names (list): List of all class names for the combined dataset.\n",
    "        output_dir (str): Output directory path for the combined dataset.\n",
    "    \"\"\"\n",
    "    # Initialize the combined class mapping\n",
    "    current_max_class = 0\n",
    "    global_image_counter = 1  # Initialize global counter for unique filenames\n",
    "\n",
    "    # Prepare the output directories\n",
    "    for folder in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(output_dir, folder, 'images'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, folder, 'labels'), exist_ok=True)\n",
    "    \n",
    "    # Process each dataset\n",
    "    for idx, dataset in enumerate(dataset_dirs):\n",
    "        print(f\"Processing dataset: {dataset}\")\n",
    "        \n",
    "        # Get the number of classes for the current dataset\n",
    "        num_classes = num_classes_list[idx]\n",
    "        \n",
    "        # Create a class mapping for the current dataset\n",
    "        class_mapping = {i: i + current_max_class for i in range(num_classes)}\n",
    "\n",
    "        for folder in ['train', 'val', 'test']:\n",
    "            image_dir = os.path.join(base_dir, dataset, folder, 'images')\n",
    "            label_dir = os.path.join(base_dir, dataset, folder, 'labels')\n",
    "\n",
    "            # Combine images and labels\n",
    "            for image_path in glob(os.path.join(image_dir, '*')):\n",
    "                # Generate a unique name using the global counter\n",
    "                unique_image_name = f\"{global_image_counter}.jpg\"\n",
    "                dest_image_path = os.path.join(output_dir, folder, 'images', unique_image_name)\n",
    "                shutil.copy(image_path, dest_image_path)\n",
    "                \n",
    "                # Find the corresponding label file and rename it\n",
    "                label_path = os.path.join(label_dir, os.path.splitext(os.path.basename(image_path))[0] + '.txt')\n",
    "                if os.path.exists(label_path):\n",
    "                    unique_label_name = f\"{global_image_counter}.txt\"  # Ensure the label name matches the image\n",
    "                    dest_label_path = os.path.join(output_dir, folder, 'labels', unique_label_name)\n",
    "\n",
    "                    # Update label content with the class mapping\n",
    "                    updated_content = update_labels(label_path, class_mapping)\n",
    "                    \n",
    "                    with open(dest_label_path, 'w') as f:\n",
    "                        f.write(updated_content)\n",
    "\n",
    "                global_image_counter += 1  # Increment global counter for the next file\n",
    "\n",
    "        # Update current_max_class for the next dataset\n",
    "        current_max_class += num_classes\n",
    "\n",
    "    # Create YAML configuration file\n",
    "    generate_yaml_file(output_dir, current_max_class, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"E:\\My Research Project\\CODE\\DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dirs = [r\"Excavators\", r\"People and Ladders\",r\"Personal Protective Equipment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"E:\\My Research Project\\CODE\\DATA\\Combined_Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_list = [3,2,14]\n",
    "class_names = [['EXCAVATORS', 'dump truck', 'wheel loader'],['Ladder', 'Person'],['Fall-Detected', 'Gloves', 'Goggles', 'Hardhat', 'Ladder', 'Mask', 'NO-Gloves', 'NO-Goggles', 'NO-Hardhat', 'NO-Mask', 'NO-Safety Vest', 'Person', 'Safety Cone', 'Safety Vest'],]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Excavators\n",
      "Processing dataset: People and Ladders\n",
      "Processing dataset: Personal Protective Equipment\n",
      "YAML configuration file created at: E:\\My Research Project\\CODE\\DATA\\Combined_Dataset\\dataset_config.yaml\n",
      "Datasets combined successfully.\n"
     ]
    }
   ],
   "source": [
    "combine_datasets(path, dataset_dirs, num_classes_list, class_names, output_dir)\n",
    "print(\"Datasets combined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AslamGPUPyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
